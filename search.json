[
  {
    "objectID": "distributed.ray.html",
    "href": "distributed.ray.html",
    "title": "Ray Backend",
    "section": "",
    "text": "RayBackend\n\n RayBackend (ray_address)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "distributed.fugue.html",
    "href": "distributed.fugue.html",
    "title": "Fugue Backend",
    "section": "",
    "text": "Fugue is a unified interface for distributed computing. The backend allows StatsForecast to fit time series using Dask and Spark."
  },
  {
    "objectID": "distributed.fugue.html#dask",
    "href": "distributed.fugue.html#dask",
    "title": "Fugue Backend",
    "section": "Dask",
    "text": "Dask\n\nfrom dask.distributed import Client\nfrom fugue_dask import DaskExecutionEngine\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import generate_series\n\ndf = generate_series(10).reset_index()\ndf['unique_id'] = df['unique_id'].astype(str)\n\ndask_client = Client()\nengine = DaskExecutionEngine(dask_client=dask_client)\nfcst = FugueBackend(engine=engine, as_local=True)\n\n\nForecast\n\nfcst.forecast(df, models=[Naive()], freq='D', h=12)\n\n\n\nCross Validation\n\nfcst.cross_validation(df, models=[Naive()], freq='D', h=12, n_windows=2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "",
    "text": "You can install the released version of StatsForecast from the Python package index with:\npip install statsforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\n\nAlso you can install the released version of StatsForecast from conda with:\nconda install -c conda-forge statsforecast\n(Installing inside a python virtualenvironment or a conda environment is recommended.)\n\n\n\nIf you want to make some modifications to the code and see the effects in real time (without reinstalling), follow the steps below:\ngit clone https://github.com/Nixtla/statsforecast.git\ncd statsforecast\npip install -e .\nTo get started just follow this guide."
  },
  {
    "objectID": "index.html#new",
    "href": "index.html#new",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üéâ New!",
    "text": "üéâ New!\n\n ETS Example: 4x faster than StatsModels with improved accuracy and robustness.\n Complete pipeline and comparison: 20x faster than pmdarima and 500x faster than Prophet."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üî• Highlights",
    "text": "üî• Highlights\n\nFastest and most accurate AutoARIMA in Python and R.\nFastest and most accurate ETS in Python and R.\nNew!: Replace FB-Prophet in two lines of code and gain speed and accuracy. Check the experiments here.\nNew!: Distributed computation in clusters with ray. (Forecast 1M series in 30min)\nNew!: Good Ol‚Äô sklearn syntax with AutoARIMA (AutoARIMA().fit(y).predict(h=7)."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üéä Features",
    "text": "üéä Features\n\nInclusion of exogenous variables and prediction intervals for ARIMA.\n20x faster than pmdarima.\n1.5x faster than R.\n500x faster than Prophet.\n4x faster than statsmodels.\nCompiled to high performance machine code through numba.\n1,000,000 series in 30 min with ray.\nOut of the box implementation of ADIDA, HistoricAverage, CrostonClassic, CrostonSBA, CrostonOptimized, SeasonalWindowAverage, SeasonalNaive, IMAPA Naive, RandomWalkWithDrift, WindowAverage, SeasonalExponentialSmoothing, TSB, AutoARIMA and ETS.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üìñ Why?",
    "text": "üìñ Why?\nCurrent Python alternatives for statistical models are slow, inaccurate and don‚Äôt scale well. So we created a library that can be used to forecast in production environments or as benchmarks. StatsForecast includes an extensive battery of models that can efficiently fit millions of time series."
  },
  {
    "objectID": "index.html#accuracy-speed",
    "href": "index.html#accuracy-speed",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üî¨ Accuracy & ‚è≤ Speed",
    "text": "üî¨ Accuracy & ‚è≤ Speed\n\nARIMA\nThe AutoARIMA model implemented in StatsForecast is 20x faster than pmdarima and 1.5x faster than R while improving accuracy. You can see the exact comparison and reproduce the results here.\n\n\nETS\nStatsForecast‚Äôs ETS is 4x faster than StatsModels‚Äô and 1.6x faster than R‚Äôs, with improved accuracy and robustness. You can see the exact comparison and reproduce the resultshere\n\n\nBenchmarks at Scale\nWith StatsForecast you can fit 9 benchmark models on 1,000,000 series in under 5 min. Reproduce the results here."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üß¨ Getting Started",
    "text": "üß¨ Getting Started\nYou can run this notebooks to get you started.\n\nExample of different AutoARIMA models on M4 data \n\nIn this notebook we present Nixtla‚Äôs AutoARIMA. The AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the alternative python implementation (pmdarima) is so slow that prevents data scientists from quickly iterating and deploying AutoARIMA in production for a large number of time series.\n\nShorter Example of fitting and AutoARIMA and an ETS model. \nBenchmarking 9 models on millions of series."
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üî® How to contribute",
    "text": "üî® How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Statistical ‚ö°Ô∏è Forecast",
    "section": "üìÉ References",
    "text": "üìÉ References\n\nThe AutoARIMA model is based (translated) from the R implementation included in the forecast package developed by Rob Hyndman.\nThe ETS model is based (translated) from the R implementation included in the forecast package developed by Rob Hyndman."
  },
  {
    "objectID": "distributed.core.html",
    "href": "distributed.core.html",
    "title": "Core",
    "section": "",
    "text": "ParallelBackend\n\n ParallelBackend ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=False, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=False, allowmean:bool=False,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            parallel:bool=False, num_cores:int=2, season_length:int=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\nOptional\nNone\nOrder of first-differencing\n\n\nD\nOptional\nNone\nOrder of seasonal-differencing\n\n\nmax_p\nint\n5\nMaximum value of p\n\n\nmax_q\nint\n5\nMaximum value of q\n\n\nmax_P\nint\n2\nMaximum value fo P\n\n\nmax_Q\nint\n2\nMaximum value of Q\n\n\nmax_order\nint\n5\nMaximum value of p+q+P+Q if model selection is not stepwise\n\n\nmax_d\nint\n2\nMaximum number of non-seasonal differences\n\n\nmax_D\nint\n1\nMaximum number of seasonal differences\n\n\nstart_p\nint\n2\nStarting value of p in stepwise procedure\n\n\nstart_q\nint\n2\nStarting value of q in stepwise procedure\n\n\nstart_P\nint\n1\nStarting value of P in stepwise procedure\n\n\nstart_Q\nint\n1\nStarting value of Q in stepwise procedure\n\n\nstationary\nbool\nFalse\nIf True, restricts search to stationary models\n\n\nseasonal\nbool\nTrue\nIf False, restricts search to non-seasonal models\n\n\nic\nstr\naicc\nInformation criterion to be used in model selection\n\n\nstepwise\nbool\nTrue\nIf False, searches over all grid (very slow)\n\n\nnmodels\nint\n94\nMaximum number of models considered in the stepwise search\n\n\ntrace\nbool\nFalse\nIf True, the list of ARIMA models considered will be reported\n\n\napproximation\nOptional\nFalse\nIf True, estimation is via conditional sums of squares\n\n\nmethod\nOptional\nNone\nmaximum likelihood or minimize conditional sum-of-squares\n\n\ntruncate\nOptional\nNone\nAn integer value indicating how many observations to use in model selection\n\n\ntest\nstr\nkpss\nType of unit root test to use\n\n\ntest_kwargs\nOptional\nNone\nAdditional arguments to be passed to the unit root test\n\n\nseasonal_test\nstr\nseas\nMethod used to select the number of seasonal differences\n\n\nseasonal_test_kwargs\nOptional\nNone\nAdditional arguments to be passed to the seasonal unit root test\n\n\nallowdrift\nbool\nFalse\nIf True, models with drift terms are considered\n\n\nallowmean\nbool\nFalse\nIf True, models with a non-zero mean are considered\n\n\nblambda\nOptional\nNone\nBox-Cox transformation parameter\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean for Box-Cox transformations\n\n\nparallel\nbool\nFalse\nIf True and stepwise = False, then the specification search is done in parallel\n\n\nnum_cores\nint\n2\nAmount of parallel processes to be used\n\n\nseason_length\nint\n1\nNumber of observations per cycle\n\n\n\nThe AutoARIMA class instantiates the model.\n\narima = AutoARIMA(season_length=12)\n\n\n\n\n\n\n AutoARIMA.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                     X_future:numpy.ndarray=None,\n                     level:Optional[Tuple[int]]=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nlevel\nOptional\nNone\nlevel\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\nOnce AutoARIMA is instantiated, you can make forecasts directly using the AutoARIMA.forecast method. The purpose of this method is to avoid computational burden due to object storage. This method can be thought of as a fit_predict without storing information. This method assumes you know the forecast horizon in advance. Observe that the method a returns a dict object instead of a numpy array.\n\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima.forecast(y=ap, h=12)\n\n\n\n\n\n\n AutoARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nOptional\nNone\nexogenous regressors\n\n\n\nIf you want to store the fitted model for future analysis, you can use the AutoARIMA.fit method.\n\narima = arima.fit(y=ap)\n\n\n\n\n\n\n AutoARIMA.predict (h:int, X:numpy.ndarray=None,\n                    level:Optional[Tuple[int]]=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nlevel\nOptional\nNone\nlevel\n\n\n\nYou can compute forecasts for the fitted model using AutoARIMA.predict.\n\narima.predict(h=12)\n\n\n\n\n\n\n AutoARIMA.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nTo obtain the fitted values, you can use AutoARIMA.predict_in_sample.\n\narima.predict_in_sample()\n\n\n\nAuto ARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Default is the corrected Akaike Information Criterion (AICc). Information criterions are tests used to check how well a model fits the data it is trying to describe.\nThe ARIMA models are based in the autocorrelations in the data and the autocorrelations of the forecast errors. Every model has three components: AR, I, and MA.\nAn AR(p) model captures the autocorrelations in the data at lags \\(1,2,\\dots p\\).\n\\[y_t = \\beta_0+\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\dots+\\beta_py{t_p}+\\epsilon_t\\]\nIf the autocorrelations of the forecast errors are added up to lag \\(q\\), an ARMA(p,q) model is obtained.\n\\[y_t = \\beta_0+\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\dots+\\beta_py{t_p} + \\epsilon_t+\\theta_1 \\epsilon_{t-1}+\\theta_2\\epsilon_{t-2}+\\dots\\theta_q\\epsilon_{t-q}\\]\nThe last component of an ARIMA model is the integrated (I) part, which is a differencing operation. The order of differencing, denoted by \\(d\\), indicates how many rounds of lag-1 differencing are performed.\nARIMA models requiere the user to select \\(p\\), \\(q\\), and \\(d\\). The values for \\(\\beta_i, i=1,2,\\dots,p\\) and \\(\\theta_j, j=1,2,\\dots,q\\) are then estimated. Auto ARIMA automatically makes this selection, searching over a range of possible values for \\(p\\), \\(q\\), and \\(d\\), and then choosing the best model using an information criterion."
  },
  {
    "objectID": "models.html#exponential-smoothing-methods",
    "href": "models.html#exponential-smoothing-methods",
    "title": "Models",
    "section": "Exponential smoothing methods",
    "text": "Exponential smoothing methods\n\n\nETS\n\n ETS (season_length:int=1, model:str='ZZZ')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per cycle\n\n\nmodel\nstr\nZZZ\nthree-character string identifying method using the framework terminology of Hyndman et al.¬†(2002)\n\n\n\n\n\n\nETS.forecast\n\n ETS.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n               X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nETS.fit\n\n ETS.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nETS.predict\n\n ETS.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nETS.predict_in_sample\n\n ETS.predict_in_sample ()\n\n\nModel description\nError, Trend, and Seasonality: Statistical models for exponential smoothing. These models are stochastic data generating processes than can produce complete forecast distributions. Each model consists of a set of equations to describe the observed data and the unobserved components or states, which are level, trend, and seasonal. Errors can be either additive or multiplicative. The notation ETS(Z,Z,Z) is used to describe the ETS model being used, where Z can take one of the following values.\n\n\n\nNotation\n\n\n\n\nN = None\n\n\nA = Additive\n\n\nAd = Additive (damped)\n\n\nM = Multiplicative\n\n\n\nThe possibilities for each state are shown below.\n\n\n\nState\nPossible values\n\n\n\n\nError\nA, M\n\n\nTrend\nN, A, Ad\n\n\nSeasonal\nN, A, M\n\n\n\n\n\n\n\nSimpleExponentialSmoothing\n\n SimpleExponentialSmoothing (alpha:float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nalpha\nfloat\nsmoothing parameter\n\n\n\n\n\n\nSimpleExponentialSmoothing.forecast\n\n SimpleExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                      X:numpy.ndarray=None,\n                                      X_future:numpy.ndarray=None,\n                                      fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSimpleExponentialSmoothing.fit\n\n SimpleExponentialSmoothing.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSimpleExponentialSmoothing.predict\n\n SimpleExponentialSmoothing.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSimpleExponentialSmoothing.predict_in_sample\n\n SimpleExponentialSmoothing.predict_in_sample ()\n\n\nModel description\nSimple (or single) exponential smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by\n\\[\\hat{y}_{t+1}= \\alpha y_{t} + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2} \\dots\\]\nwhich can also be written as\n\\[\\hat{y}_{t+1} = \\alpha y_t + \\alpha(1-\\alpha) \\hat{y}_{t-1}\\]\nThe rate \\(0 \\leq \\alpha \\leq 1\\) at which the weights decrease is called the smoothing parameter. When \\(\\alpha = 1\\), SES is equal to the naive method.\n\n\n\n\nSimpleExponentialSmoothingOptimized\n\n SimpleExponentialSmoothingOptimized ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nSimpleExponentialSmoothingOptimized.forecast\n\n SimpleExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                               X:numpy.ndarray=None, X_fut\n                                               ure:numpy.ndarray=None,\n                                               fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSimpleExponentialSmoothingOptimized.fit\n\n SimpleExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                          X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSimpleExponentialSmoothingOptimized.predict\n\n SimpleExponentialSmoothingOptimized.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSimpleExponentialSmoothingOptimized.predict_in_sample\n\n SimpleExponentialSmoothingOptimized.predict_in_sample ()\n\n\nModel description\nSimple exponential smoothing optimized: A version of SES where the smoothing parameter \\(\\alpha\\) is chosen automatically by minimizing the mean squared error of the fit.\n\n\n\n\nSeasonalExponentialSmoothing\n\n SeasonalExponentialSmoothing (season_length:int, alpha:float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nseason_length\nint\nNumber of observations per cycle\n\n\nalpha\nfloat\nsmoothing parameter shared across seasonalities\n\n\n\n\n\n\nSeasonalExponentialSmoothing.forecast\n\n SeasonalExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                        X:numpy.ndarray=None,\n                                        X_future:numpy.ndarray=None,\n                                        fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSeasonalExponentialSmoothing.fit\n\n SeasonalExponentialSmoothing.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalExponentialSmoothing.predict\n\n SeasonalExponentialSmoothing.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalExponentialSmoothing.predict_in_sample\n\n SeasonalExponentialSmoothing.predict_in_sample ()\n\n\nModel description\nSeasonal exponential smoothing: A seasonal version of exponential smoothing. A simple exponential smoothing is fitted for each seasonality using the smoothing parameter \\(\\alpha\\).\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n\n SeasonalExponentialSmoothingOptimized (season_length:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nseason_length\nint\nNumber of observations per cycle\n\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized.forecast\n\n SeasonalExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                                 X:numpy.ndarray=None, X_f\n                                                 uture:numpy.ndarray=None,\n                                                 fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized.fit\n\n SeasonalExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                            X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized.predict\n\n SeasonalExponentialSmoothingOptimized.predict (h:int,\n                                                X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized.predict_in_sample\n\n SeasonalExponentialSmoothingOptimized.predict_in_sample ()\n\n\nModel description\nSeasonal SES optimized: A seasonal version of SES optimized"
  },
  {
    "objectID": "models.html#simple-methods",
    "href": "models.html#simple-methods",
    "title": "Models",
    "section": "Simple methods",
    "text": "Simple methods\n\n\nHistoricAverage\n\n HistoricAverage ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nHistoricAverage.forecast\n\n HistoricAverage.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                           X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nHistoricAverage.fit\n\n HistoricAverage.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nHistoricAverage.predict\n\n HistoricAverage.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nHistoricAverage.predict_in_sample\n\n HistoricAverage.predict_in_sample ()\n\n\nModel description\nHistoric average: Also known as mean method. Uses a simple average of all past observations. Assuming there are \\(t\\) observations, the one-step forecast is given by\n\\[ \\hat{y}_{t+1} = \\frac{1}{t} \\sum_{j=1}^t y_j \\]\n\n\n\n\nNaive\n\n Naive ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nNaive.forecast\n\n Naive.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                 X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nNaive.fit\n\n Naive.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nNaive.predict\n\n Naive.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nNaive.predict_in_sample\n\n Naive.predict_in_sample ()\n\n\nModel description\nNaive: A random walk model, defined as\n\\[ \\hat{y}_{t+1} = y_t \\]\n\n\n\n\nRandomWalkWithDrift\n\n RandomWalkWithDrift ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nRandomWalkWithDrift.forecast\n\n RandomWalkWithDrift.forecast (y:numpy.ndarray, h:int,\n                               X:numpy.ndarray=None,\n                               X_future:numpy.ndarray=None,\n                               fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nRandomWalkWithDrift.fit\n\n RandomWalkWithDrift.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nRandomWalkWithDrift.predict\n\n RandomWalkWithDrift.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nRandomWalkWithDrift.predict_in_sample\n\n RandomWalkWithDrift.predict_in_sample ()\n\n\nModel description\nRandom walk with drift: A variation of the naive method allows the forecasts to change over time. The amout of change, called drift, is the average change seen in the historical data.\n\\[ \\hat{y}_{t+1} = y_t+\\frac{1}{t-1}\\sum_{j=1}^t (y_j-y_{j-1}) = y_t+ \\frac{y_t-y_1}{t-1} \\]\nFrom the previous equation, we can see that this is equivalent to extrapolating a line between the first and the last observation.\n\n\n\n\nSeasonalNaive\n\n SeasonalNaive (season_length:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nseason_length\nint\nNumber of observations per cycle\n\n\n\n\n\n\nSeasonalNaive.forecast\n\n SeasonalNaive.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                         X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSeasonalNaive.fit\n\n SeasonalNaive.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalNaive.predict\n\n SeasonalNaive.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalNaive.predict_in_sample\n\n SeasonalNaive.predict_in_sample ()\n\n\nModel description\nSeasonal naive: Similar to the naive method, but uses the last known observation of the same period (e.g.¬†the same month of the previous year) in order to capture seasonal variations.\n\n\n\n\nWindowAverage\n\n WindowAverage (window_size:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nwindow_size\nint\nlast observations used to compute average\n\n\n\n\n\n\nWindowAverage.forecast\n\n WindowAverage.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                         X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nWindowAverage.fit\n\n WindowAverage.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nWindowAverage.predict\n\n WindowAverage.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nWindowAverage.predict_in_sample\n\n WindowAverage.predict_in_sample ()\n\n\nModel description\nWindow average: Uses the average of the last \\(k\\) observations, with \\(k\\) the length of the window. Wider windows will capture global trends, while narrow windows will reveal local trends. The length of the window selected should take into account the importance of past observations and how fast the series changes.\n\n\n\n\nSeasonalWindowAverage\n\n SeasonalWindowAverage (season_length:int, window_size:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nseason_length\nint\nNumber of observations per cycle\n\n\nwindow_size\nint\nLast observations per seasonality to compute average\n\n\n\n\n\n\nSeasonalWindowAverage.forecast\n\n SeasonalWindowAverage.forecast (y:numpy.ndarray, h:int,\n                                 X:numpy.ndarray=None,\n                                 X_future:numpy.ndarray=None,\n                                 fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nSeasonalWindowAverage.fit\n\n SeasonalWindowAverage.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalWindowAverage.predict\n\n SeasonalWindowAverage.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nSeasonalWindowAverage.predict_in_sample\n\n SeasonalWindowAverage.predict_in_sample ()\n\n\nModel description\nSeasonal window average: An average of the last \\(k\\) observations of the same period, with \\(k\\) the length of the window."
  },
  {
    "objectID": "models.html#sparse-or-intermittent-series",
    "href": "models.html#sparse-or-intermittent-series",
    "title": "Models",
    "section": "Sparse or intermittent series",
    "text": "Sparse or intermittent series\nSparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them. Before the development of Croston‚Äôs method and its variants, SES was usually used to forecast them.\n\n\nADIDA\n\n ADIDA ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nADIDA.forecast\n\n ADIDA.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                 X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nADIDA.fit\n\n ADIDA.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nADIDA.predict\n\n ADIDA.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nADIDA.predict_in_sample\n\n ADIDA.predict_in_sample ()\n\n\nModel description\nAggregate-Dissagregate Intermittent Demand Approach: Uses temporal aggregation to reduce the number of zero observations. Once the data has been agregated, it uses the optimized SES to generate the forecasts at the new level. It then breaks down the forecast to the original level using equal weights.\n\n\n\n\nCrostonClassic\n\n CrostonClassic ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nCrostonClassic.forecast\n\n CrostonClassic.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                          X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nCrostonClassic.fit\n\n CrostonClassic.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nCrostonClassic.predict\n\n CrostonClassic.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nCrostonClassic.predict_in_sample\n\n CrostonClassic.predict_in_sample (level)\n\n\nModel description\nCroston classic: A method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by\n\\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nwhere \\(\\hat{z}_t\\) and \\(\\hat{p}_t\\) are forecasted using SES. The smoothing parameter of both components is set equal to 0.1\n\n\n\n\nCrostonOptimized\n\n CrostonOptimized ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nCrostonOptimized.forecast\n\n CrostonOptimized.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                            X_future:numpy.ndarray=None,\n                            fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nCrostonOptimized.fit\n\n CrostonOptimized.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nCrostonOptimized.predict\n\n CrostonOptimized.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nCrostonOptimized.predict_in_sample\n\n CrostonOptimized.predict_in_sample ()\n\n\nModel description\nCroston Optimized: A variation of the classic Croston‚Äôs method where the smooting paramater is optimally selected from the range \\([0.1,0.3]\\). Both the non-zero demand \\(z_t\\) and the inter-demand intervals \\(p_t\\) are smoothed separately, so their smoothing parameters can be different.\n\n\n\n\nCrostonSBA\n\n CrostonSBA ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nCrostonSBA.forecast\n\n CrostonSBA.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                      X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nCrostonSBA.fit\n\n CrostonSBA.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\n# exogenous regressors\n\n\n\n\n\n\nCrostonSBA.predict\n\n CrostonSBA.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nCrostonSBA.predict_in_sample\n\n CrostonSBA.predict_in_sample ()\n\n\nModel description\nCroston with Syntetos-Boylan Approximation: A variation of the classic Croston‚Äôs method that uses a debiasing factor, so that the forecast is given by\n\\[ \\hat{y}_t = 0.95  \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\n\n\n\n\nIMAPA\n\n IMAPA ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nIMAPA.forecast\n\n IMAPA.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n                 X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nIMAPA.fit\n\n IMAPA.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nIMAPA.predict\n\n IMAPA.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nIMAPA.predict_in_sample\n\n IMAPA.predict_in_sample ()\n\n\nModel description\nIntermittent Multiple Aggregation Prediction Algorithm: Similar to ADIDA, but instead of using a single aggregation level, it considers multiple in order to capture different dynamics of the data. Uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\n\n\n\n\nTSB\n\n TSB (alpha_d:float, alpha_p:float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nalpha_d\nfloat\nsmoothing parameter for demand\n\n\nalpha_p\nfloat\nsmoothing parameter for probability\n\n\n\n\n\n\nTSB.forecast\n\n TSB.forecast (y:numpy.ndarray, h:int, X:numpy.ndarray=None,\n               X_future:numpy.ndarray=None, fitted:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\nX_future\nndarray\nNone\nfuture regressors\n\n\nfitted\nbool\nFalse\nreturn fitted values?\n\n\n\n\n\n\nTSB.fit\n\n TSB.fit (y:numpy.ndarray, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\ntime series\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nTSB.predict\n\n TSB.predict (h:int, X:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\nndarray\nNone\nexogenous regressors\n\n\n\n\n\n\nTSB.predict_in_sample\n\n TSB.predict_in_sample ()\n\n\nModel description\nTeunter-Syntetos-Babai: A modification of Croston‚Äôs method that replaces the inter-demand intervals with the demand probability \\(d_t\\), which is defined as follows.\n\\[\nd_t = \\begin{cases}\n    1  & \\text{if demand occurs at time t}\\\\\n    0 & \\text{otherwise.}\n\\end{cases}\n\\]\nHence, the forecast is given by\n\\[\\hat{y}_t= \\hat{d}_t\\hat{z_t}\\]\nBoth \\(d_t\\) and \\(z_t\\) are forecasted using SES. The smooting paramaters of each may differ, like in the optimized Croston‚Äôs method."
  },
  {
    "objectID": "models.html#references",
    "href": "models.html#references",
    "title": "Models",
    "section": "References",
    "text": "References\n\nGeneral\n\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on July 2022.\nShmueli, G., & Lichtendahl Jr, K. C. (2016). Practical time series forecasting with r: A hands-on guide. Axelrod Schnall Publishers.\n\n\n\nFor sparse or intermittent series\n\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\nNikolopoulos, K., Syntetos, A. A., Boylan, J. E., Petropoulos, F., & Assimakopoulos, V. (2011). An aggregate‚Äìdisaggregate intermittent demand approach (ADIDA) to forecasting: an empirical proposition and analysis. Journal of the Operational Research Society, 62(3), 544-554.\nSyntetos, A. A., & Boylan, J. E. (2005). The accuracy of intermittent demand estimates. International Journal of forecasting, 21(2), 303-314.\nSyntetos, A. A., & Boylan, J. E. (2021). Intermittent demand forecasting: Context, methods and applications. John Wiley & Sons.\nTeunter, R. H., Syntetos, A. A., & Babai, M. Z. (2011). Intermittent demand: Linking forecasting to inventory obsolescence. European Journal of Operational Research, 214(3), 606-615.\n\n\nimport pandas as pd\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = AutoARIMA(season_length=12)\narima = arima.fit(ap)\narima.predict(12)\n\n\nets = ETS(season_length=12)\nets = ets.fit(ap)\nets.predict(12)\n\nExternal regressors\n\ndrift = np.arange(1, ap.size + 1)\nX = np.vstack([np.log(drift), np.sqrt(drift)]).T\n\n\nnewdrift = np.arange(ap.size + 1, ap.size + 7 + 1).reshape(-1, 1)\nnewxreg = np.concatenate([np.log(newdrift), np.sqrt(newdrift)], axis=1)\n\n\narima = AutoARIMA(season_length=12)\narima = arima.fit(y=ap, X=X)\narima.predict(12, X=newxreg)\n\nConfidence intervals\n\npd.DataFrame(arima.predict(12, X=newxreg, level=(80, 50, 95))).plot()"
  },
  {
    "objectID": "arima.html#parameters",
    "href": "arima.html#parameters",
    "title": "ARIMA",
    "section": "Parameters",
    "text": "Parameters\nd: int optional (default None) Order of first-differencing. If missing, will choose a value based on test. D: int optional (default None) Order of seasonal-differencing. If missing, will choose a value based on season_test. max_p: int (default 5) Maximum value of p. max_q: int (default 5) Maximum value of q. max_P: int (default 2) Maximum value of P. max_Q: int (default 2) Maximum value of Q. max_order: int (default 5) Maximum value of p+q+P+Q if model selection is not stepwise. max_d: int (default 2) Maximum number of non-seasonal differences max_D: int (default 1) Maximum number of seasonal differences start_p: int (default 2) Starting value of p in stepwise procedure. start_q: int (default 2) Starting value of q in stepwise procedure. start_P: int (default 1) Starting value of P in stepwise procedure. start_Q: int (default 1) Starting value of Q in stepwise procedure. stationary: bool (default False) If True, restricts search to stationary models. seasonal: bool (default True) If False, restricts search to non-seasonal models. ic: str (default ‚Äòaicc‚Äô) Information criterion to be used in model selection. stepwise: bool (default True) If True, will do stepwise selection (faster). Otherwise, it searches over all models. Non-stepwise selection can be very slow, especially for seasonal models. nmodels: int (default 94) Maximum number of models considered in the stepwise search. trace: bool (default False) If True, the list of ARIMA models considered will be reported. approximation: bool optional (default None) If True, estimation is via conditional sums of squares and the information criteria used for model selection are approximated. The final model is still computed using maximum likelihood estimation. Approximation should be used for long time series or a high seasonal period to avoid excessive computation times. method: str optional (default None) fitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood. Can be abbreviated. truncate: bool optional (default None) An integer value indicating how many observations to use in model selection. The last truncate values of the series are used to select a model when truncate is not None and approximation=True. All observations are used if either truncate=None or approximation=False. test: str (default ‚Äòkpss‚Äô) Type of unit root test to use. See ndiffs for details. test_kwargs: str optional (default None) Additional arguments to be passed to the unit root test. seasonal_test: str (default ‚Äòseas‚Äô) This determines which method is used to select the number of seasonal differences. The default method is to use a measure of seasonal strength computed from an STL decomposition. Other possibilities involve seasonal unit root tests. seasonal_test_kwargs: dict optional (default None) Additional arguments to be passed to the seasonal unit root test. See nsdiffs for details. allowdrift: bool (default True) If True, models with drift terms are considered. allowmean: bool (default True) If True, models with a non-zero mean are considered. blambda: float optional (default None) Box-Cox transformation parameter. If lambda=‚Äúauto‚Äù, then a transformation is automatically selected using BoxCox.lambda. The transformation is ignored if None. Otherwise, data transformed before model is estimated. biasadj: bool (default False) Use adjusted back-transformed mean for Box-Cox transformations. If transformed data is used to produce forecasts and fitted values, a regular back transformation will result in median forecasts. If biasadj is True, an adjustment will be made to produce mean forecasts and fitted values. parallel: bool (default False) If True and stepwise = False, then the specification search is done in parallel. This can give a significant speedup on multicore machines. num_cores: int (default 2) Allows the user to specify the amount of parallel processes to be used if parallel = True and stepwise = False. If None, then the number of logical cores is automatically detected and all available cores are used. period: int (default 1) Number of observations per unit of time. For example 24 for Hourly data."
  },
  {
    "objectID": "arima.html#notes",
    "href": "arima.html#notes",
    "title": "ARIMA",
    "section": "Notes",
    "text": "Notes\n\nThis implementation is a mirror of Hyndman‚Äôs forecast::auto.arima."
  },
  {
    "objectID": "arima.html#references",
    "href": "arima.html#references",
    "title": "ARIMA",
    "section": "References",
    "text": "References\n[1] https://github.com/robjhyndman/forecast"
  },
  {
    "objectID": "distributed.utils.html",
    "href": "distributed.utils.html",
    "title": "Distributed utils",
    "section": "",
    "text": "forecast\n\n forecast (df, models, freq, h, X_df=None, level=None,\n           parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\n\n\ncross_validation\n\n cross_validation (df, models, freq, h, n_windows=1, step_size=1,\n                   test_size=None, input_size=None,\n                   parallel:Optional[ForwardRef('ParallelBackend')]=None)"
  },
  {
    "objectID": "distributed.multiprocess.html",
    "href": "distributed.multiprocess.html",
    "title": "Multiprocess Backend",
    "section": "",
    "text": "MultiprocessBackend\n\n MultiprocessBackend (n_jobs:int)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "adapters.prophet.html",
    "href": "adapters.prophet.html",
    "title": "Adapters for Prophet",
    "section": "",
    "text": "AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False,\n                   parallel=False, num_cores=2, period=1)\n\nReturns best ARIMA model using external variables created by the Prophet interface.\nThis class receives as parameters the same as prophet.Prophet and statsforecast.arima.AutoARIMA.\nIf your pipeline uses Prophet you can simply replace Prophet with AutoARIMAProphet and you‚Äôll be using AutoARIMA instead of Prophet.\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')\n\n\n\nUsually, a Prophet pipeline without external regressors looks like this.\n\nm = Prophet()\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\nfig = m.plot(forecast)\n\nWith the class AutoARIMAProphet you can simply replace Prophet and you‚Äôll be training an auto_arima model without changing the pipeline.\n\nm = AutoARIMAProphet()\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\n\n\n\n\nUsually Prophet pipelines include the usage of external regressors such as holidays.\n\nplayoffs = pd.DataFrame({\n  'holiday': 'playoff',\n  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n                        '2010-01-24', '2010-02-07', '2011-01-08',\n                        '2013-01-12', '2014-01-12', '2014-01-19',\n                        '2014-02-02', '2015-01-11', '2016-01-17',\n                        '2016-01-24', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((playoffs, superbowls))\n\n\nm = Prophet(holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nThe class AutoARIMAProphet allows you to handle these scenarios to fit an auto_arima model with exogenous variables.\n\nm = AutoARIMAProphet(holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "generate_series\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, seed:int=0)\n\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_static_features > 0, then each serie gets static features with random values. If equal_ends == True then all series end at the same date.\n\ndata = generate_series(100)\ndata\n\n\nmonthly_data = generate_series(100, freq='M')\nmonthly_data\n\n\n\nAirPassengers data"
  },
  {
    "objectID": "ets.html",
    "href": "ets.html",
    "title": "ETS Model",
    "section": "",
    "text": "ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)"
  },
  {
    "objectID": "core.html#integer-datestamp",
    "href": "core.html#integer-datestamp",
    "title": "Core",
    "section": "Integer datestamp",
    "text": "Integer datestamp\nThe StatsForecast class can also receive integers as datestamp, the following example shows how to do it.\n\nfrom statsforecast.utils import AirPassengers as ap\n\n\nint_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\nint_ds_df.insert(0, 'unique_id', 'AirPassengers')\nint_ds_df.set_index('unique_id', inplace=True)\nint_ds_df.head()\n\n\nint_ds_df.tail()\n\n\nfcst = StatsForecast(df=int_ds_df, models=[HistoricAverage()], freq='D')\nhorizon = 7\nforecast = fcst.forecast(horizon)\nforecast.head()\n\n\nlast_date = int_ds_df['ds'].max()\ntest_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))\n\n\nint_ds_cv = fcst.cross_validation(h=7, test_size=8, n_windows=None)\nint_ds_cv"
  },
  {
    "objectID": "core.html#external-regressors",
    "href": "core.html#external-regressors",
    "title": "Core",
    "section": "External regressors",
    "text": "External regressors\nEvery column after y is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the StatsForecast.forecast method.\n\nclass LinearRegression:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, y, X):\n        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return self\n    \n    def predict(self, h, X):\n        mean = X @ coefs\n        return mean\n    \n    def __repr__(self):\n        return 'LinearRegression()'\n    \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return {'mean': X_future @ coefs}\n    \n    def new(self):\n        b = type(self).__new__(type(self))\n        b.__dict__.update(self.__dict__)\n        return b\n\n\nseries_xreg = series = generate_series(10_000, equal_ends=True)\nseries_xreg['intercept'] = 1\nseries_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\nseries_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\nseries_xreg\n\n\ndates = sorted(series_xreg['ds'].unique())\nvalid_start = dates[-14]\ntrain_mask = series_xreg['ds'] < valid_start\nseries_train = series_xreg[train_mask]\nseries_valid = series_xreg[~train_mask]\nX_valid = series_valid.drop(columns=['y'])\nfcst = StatsForecast(\n    df=series_train,\n    models=[LinearRegression()],\n    freq='D',\n)\nxreg_res = fcst.forecast(14, X_df=X_valid)\nxreg_res['y'] = series_valid['y'].values\n\n\nxreg_res.groupby('ds').mean().plot()\n\n\nxreg_res_cv = fcst.cross_validation(h=3, test_size=5, n_windows=None)"
  },
  {
    "objectID": "core.html#confidence-intervals",
    "href": "core.html#confidence-intervals",
    "title": "Core",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nYou can pass the argument level to the StatsForecast.forecast method to calculate confidence intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented.\n\nap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\nfcst = StatsForecast(\n    df=ap_df,\n    models=[\n        SeasonalNaive(season_length=12), \n        AutoARIMA(season_length=12)\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = fcst.forecast(12, level=(80, 95))\nap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\n\n\n#hide\ndef test_conf_intervals(n_jobs=1):\n    ap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\n    fcst = StatsForecast(\n        df=ap_df,\n        models=[\n            SeasonalNaive(season_length=12), \n            AutoARIMA(season_length=12)\n        ],\n        freq='M',\n        n_jobs=n_jobs\n    )\n    ap_ci = fcst.forecast(12, level=(80, 95))\n    ap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\ntest_conf_intervals(n_jobs=1)"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "",
    "text": "Automatic forecasting tools tackle the needs for predictions over large collections of univariate time series that often arise in business practice and other contexts. Among these solutions, R‚Äôs forecasting package auto.arima and ets has been a reference for their accuracy and high quality for many years.\nUnfortunately, baselines with their accuracy and computational efficiency were not available for Python yet. For this reason, we developed our new and highly efficient pure-Python implementation of these classic algorithms that we showcase in this notebook.\nIf you are interested in talking about this or other time series models or want to sclae your models in production environments don‚Äôt hesitate to send us an email at hello[at]nixtla.io or join our slack community.\nHyndman, RJ and Khandakar, Y (2008) ‚ÄúAutomatic time series forecasting: The forecast package for R‚Äù, Journal of Statistical Software, 26(3)."
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#installing-statsforecast-library",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#installing-statsforecast-library",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install -U numba\n!pip install -U statsmodels\n!pip install statsforecast\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, ETS\nfrom statsforecast.utils import AirPassengersDF\n\nIf you want to list all avaiaible models run the following lines,\n\n#from statsforecast.models import __all__\n#__all__"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#loading-airpassengers-example-data",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#loading-airpassengers-example-data",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Loading AirPassengers Example Data",
    "text": "Loading AirPassengers Example Data\n\n# We define the train df. \n# We use the index functionality to make the training a lot faster.\nY_df = AirPassengersDF\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#fit-autoarima-and-autoets",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#fit-autoarima-and-autoets",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Fit AutoArima and AutoETS",
    "text": "Fit AutoArima and AutoETS\nETS: The exponential smoothing (ETS) algorithm is especially suited for data with seasonality and trend. ETS computes a weighted average over all observations in the input time series dataset as its prediction. In contrast to moving average methods with constant weights, ETS weights exponentially decrease over time, capturing long term dependencies while prioritizing new observations.\nAutoARIMA: The autoregressive integrated moving average (ARIMA), combines differencing steps, lag regression and moving averages into a single method capable of modeling non-stationary time series. This method complements on ETS and it is based on the description of data‚Äôs autocorrelations.\n\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 test\n\nDefine the parameters that you want to use in your models. For ETS we pass a ZMZ, model, which stands for error and trend kinds selected optimally. In this step, you could include further models like: SeasonalExponentialSmoothing, ADIDA, HistoricAverage, CrostonClassic, CrostonSBA, CrostonOptimized, SeasonalWindowAverage, SeasonalNaive, IMAPA, Naive, RandomWalkWithDrift, WindowAverage, SeasonalExponentialSmoothing, and TSB.\n\nseason_length = 12\nhorizon = len(Y_test_df)\nmodels = [\n    AutoARIMA(season_length=season_length),\n    ETS(season_length=season_length, model='ZMZ')\n]\nmodel = StatsForecast(\n    df=Y_train_df, \n    models=models,\n    freq='M', \n    n_jobs=-1,\n)\n\nY_hat_df = model.forecast(horizon).reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      ETS\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      424.160156\n      419.163574\n    \n    \n      1\n      1.0\n      1960-02-29\n      407.081696\n      416.904449\n    \n    \n      2\n      1.0\n      1960-03-31\n      470.860535\n      480.243378\n    \n    \n      3\n      1.0\n      1960-04-30\n      460.913605\n      461.996887\n    \n    \n      4\n      1.0\n      1960-05-31\n      484.900879\n      463.853241"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#plot-and-evaluate-predictions",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#plot-and-evaluate-predictions",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Plot and Evaluate Predictions",
    "text": "Plot and Evaluate Predictions\nWe are going to plot the models againts the real values of test.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\n\nplot_df[['y', 'AutoARIMA', 'ETS']].plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\nFinally, we evaluate the predictions accuracy using the Mean Absolute Error:\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\ny_true = Y_test_df['y'].values\nets_preds = Y_hat_df['ETS'].values\narima_preds = Y_hat_df['AutoARIMA'].values\n\nprint('ETS   MAE: %0.3f' % mae(ets_preds, y_true))\nprint('ARIMA MAE: %0.3f' % mae(arima_preds, y_true))\n\nETS   MAE: 16.222\nARIMA MAE: 18.551"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#add-confidence-intervals-to-arima",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#add-confidence-intervals-to-arima",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Add Confidence Intervals to ARIMA",
    "text": "Add Confidence Intervals to ARIMA\nYou just need to add the level argument to the StatsForecast.forecast method as follows,\n\nY_hat_df_intervals = model.forecast(h=12, level=(80, 95))\n\nThen we plot the intervals,\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([Y_train_df, Y_hat_df_intervals]).set_index('ds')\ndf_plot[['y', 'AutoARIMA','ETS']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['AutoARIMA-lo-80'], \n                df_plot['AutoARIMA-hi-80'],\n                alpha=.35,\n                color='orange',\n                label='auto_arima_level_80')\nax.fill_between(df_plot.index, \n                df_plot['AutoARIMA-lo-95'], \n                df_plot['AutoARIMA-hi-95'],\n                alpha=.2,\n                color='orange',\n                label='auto_arima_level_95')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#add-external-regressors-to-arima",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#add-external-regressors-to-arima",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Add external regressors to ARIMA",
    "text": "Add external regressors to ARIMA\nFirst we are going to include new exogenous variables as columns to our train data frame. (You can include things like weather or holidays.)\n\nY_train_df['trend'] = np.arange(1, len(Y_train_df) + 1)\nY_train_df['intercept'] = np.ones(len(Y_train_df))\nY_train_df['month'] = Y_train_df['ds'].dt.month\nY_train_df = pd.get_dummies(Y_train_df, columns=['month'], drop_first=True)\n\n\nY_train_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      trend\n      intercept\n      month_2\n      month_3\n      month_4\n      month_5\n      month_6\n      month_7\n      month_8\n      month_9\n      month_10\n      month_11\n      month_12\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n      1\n      1.0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n      2\n      1.0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n      3\n      1.0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n      4\n      1.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n      5\n      1.0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\nWe consruct the test dataframe of exogenous variables.\n\n# \nxreg_test = pd.DataFrame({\n  'unique_id': 1,\n  'ds': pd.date_range(start='1960-01-01', periods=len(Y_hat_df), freq='M')\n})\n# We construct xreg for test. The train series ends at the 133th step. \nxreg_test['trend'] = np.arange(133, len(Y_hat_df) + 133)\nxreg_test['intercept'] = np.ones(len(Y_hat_df))\nxreg_test['month'] = xreg_test['ds'].dt.month\nxreg_test = pd.get_dummies(xreg_test, columns=['month'], drop_first=True)\nxreg_test.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      trend\n      intercept\n      month_2\n      month_3\n      month_4\n      month_5\n      month_6\n      month_7\n      month_8\n      month_9\n      month_10\n      month_11\n      month_12\n    \n  \n  \n    \n      0\n      1\n      1960-01-31\n      133\n      1.0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      1960-02-29\n      134\n      1.0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      1960-03-31\n      135\n      1.0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      1\n      1960-04-30\n      136\n      1.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      1960-05-31\n      137\n      1.0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nseason_length = 12\nmodel = StatsForecast(\n    df=Y_train_df, \n    models=models, \n    freq='M', \n    n_jobs=-1\n)\n\nY_hat_df_xreg = model.forecast(horizon, X_df=xreg_test)\nY_hat_df_xreg = Y_hat_df_xreg.reset_index()\n\nWe are going to plot the models againts the real values of test.\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df_xreg = Y_test_df.merge(Y_hat_df_xreg, how='left', on=['unique_id', 'ds'])\ndf_plot = pd.concat([Y_train_df, Y_hat_df_xreg]).set_index('ds')\ndf_plot[['y', 'AutoARIMA','ETS']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast (with AutoArima external regressors)', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)"
  },
  {
    "objectID": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#include-other-benchmark-models",
    "href": "examples/Getting_Started_with_Auto_Arima_and_ETS.html#include-other-benchmark-models",
    "title": "Getting Started with AutoARIMA and ETS",
    "section": "Include other Benchmark models",
    "text": "Include other Benchmark models\nWe import more benchmark models as follows,\n\nfrom statsforecast.models import SeasonalNaive, Naive\n\n\nseason_length = 12\nmodels = [\n    AutoARIMA(season_length=12),\n    ETS(season_length=12),\n    SeasonalNaive(season_length=12),\n    Naive()\n]\nmodel = StatsForecast(\n    df=Y_train_df, \n    models=models, \n    freq='M', \n    n_jobs=-1\n)\n\nY_hat_df_bench = model.forecast(horizon, X_df=xreg_test)\nY_hat_df_bench = Y_hat_df_bench.reset_index()\n\nWe are going to plot the models againts the real values of test,\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df_bench = Y_test_df.merge(Y_hat_df_bench, how='left', on=['unique_id', 'ds'])\ndf_plot = pd.concat([Y_train_df, Y_hat_df_bench]).set_index('ds')\ndf_plot[['y', 'AutoARIMA', 'ETS', 'SeasonalNaive', 'Naive']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast (with AutoArima external regressors)', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)"
  },
  {
    "objectID": "examples/IntermittentData.html",
    "href": "examples/IntermittentData.html",
    "title": "Forecasting Intermitent Time Series",
    "section": "",
    "text": "Sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nIn this notebook we‚Äôll implement and benchmark some of the most popular methods for intermitent forecast using the StatsForecast library."
  },
  {
    "objectID": "examples/IntermittentData.html#installing-statsforecast-library",
    "href": "examples/IntermittentData.html#installing-statsforecast-library",
    "title": "Forecasting Intermitent Time Series",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install -U numba\n!pip install -U statsmodels\n!pip install statsforecast\n!pip install neuralforecast\n\n\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom neuralforecast.data.datasets import m5\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    ADIDA, CrostonClassic, CrostonOptimized,\n    CrostonSBA, IMAPA, TSB, ETS, AutoARIMA\n)\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\n\n\nPlot functions\n\ndef plot_grid(df_train, plot_titles, model_cols=[\"y_50\"], df_test=None, plot_random=True):\n    \"\"\"Plots multiple time series.\"\"\"\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train', c='black')\n        axes[idx, idy].xaxis.set_tick_params(rotation=45)\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y'], c='black', label='True')\n\n            for col in model_cols:\n                axes[idx, idy].plot(test_uid['ds'], test_uid[col], label=col)\n\n        axes[idx, idy].set_title(f'State: {plot_titles[uid]}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.7)\n    plt.show()"
  },
  {
    "objectID": "examples/IntermittentData.html#loading-and-exploring-the-m5-dataset",
    "href": "examples/IntermittentData.html#loading-and-exploring-the-m5-dataset",
    "title": "Forecasting Intermitent Time Series",
    "section": "Loading and Exploring the M5 Dataset",
    "text": "Loading and Exploring the M5 Dataset\nThe M5 dataset consists of real-life data from Walmart and was used on a competition conducted on Kaggle. The data consists of around 42,000 hierarchical daily time series, starting at the level of SKUs and ending with the total demand of some large geographical area.\n\nY_df_total, *_ = m5.M5.load('./data')\n\n\nprint(Y_df_total['unique_id'].nunique())\nY_df_total.head()\n\n30490\n\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\n\nPrepare dataset for StatsForecast modelling\n\nCreate a subset of the dataset\n\nids = random.sample(list(Y_df_total['unique_id'].unique()), 100)\n\nY_df = Y_df_total.query('unique_id in @ids')\n\nY_df[\"unique_id\"] = Y_df[\"unique_id\"].astype(str)\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      573344\n      FOODS_1_035_CA_1\n      2011-01-29\n      3.0\n    \n    \n      573345\n      FOODS_1_035_CA_1\n      2011-01-30\n      5.0\n    \n    \n      573346\n      FOODS_1_035_CA_1\n      2011-01-31\n      1.0\n    \n    \n      573347\n      FOODS_1_035_CA_1\n      2011-02-01\n      0.0\n    \n    \n      573348\n      FOODS_1_035_CA_1\n      2011-02-02\n      0.0\n    \n  \n\n\n\n\n\nplot_titles = dict(zip(ids, ids))\nplot_grid(Y_df, plot_titles=plot_titles)"
  },
  {
    "objectID": "examples/IntermittentData.html#modelling-intermitent-data",
    "href": "examples/IntermittentData.html#modelling-intermitent-data",
    "title": "Forecasting Intermitent Time Series",
    "section": "Modelling intermitent data",
    "text": "Modelling intermitent data\n\nADIDA: Temporal aggregation is used for reducing the presence of zero observations, thus mitigating the undesirable effect of the variance observed in the intervals. ADIDA uses equally sized time buckets to perform non-overlapping temporal aggregation and predict the demand over a pre-specified lead-time.\niMAPA: iMAPA stands for Intermittent Multiple Aggregation Prediction Algorithm. Another way for implementing temporal aggregation in demand forecasting. However, in contrast to ADIDA that considers a single aggregation level, iMAPA considers multiple ones, aiming at capturing different dynamics of the data\nTSB: TSB stands for Teunter-Syntetos-Babai. A modification to Croston‚Äôs method that replaces the inter-demand intervals component with the demand probability.\n\n\n# Split train test\nY_train_df = Y_df[Y_df.ds <= '2016-06-12']\nY_test_df = Y_df[Y_df.ds > '2016-06-12']\nprint(f\"Train: {Y_train_df.ds.nunique()}\")\nprint(f\"Test: {Y_test_df.ds.nunique()}\")\n\nTrain: 1962\nTest: 7\n\n\n\nmodels = [ADIDA(), IMAPA(), TSB(alpha_d=0.2, alpha_p=0.2)]\nhorizon = 7\nfreq = \"D\"\n\n\n# Create the forecast object and forecast test set\nmodel = StatsForecast(df=Y_train_df, models=models, freq=freq, n_jobs=-1)\n\nY_hat_df = model.forecast(horizon).reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      ADIDA\n      IMAPA\n      TSB\n    \n  \n  \n    \n      0\n      FOODS_1_035_CA_1\n      2016-06-13\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      1\n      FOODS_1_035_CA_1\n      2016-06-14\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      2\n      FOODS_1_035_CA_1\n      2016-06-15\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      3\n      FOODS_1_035_CA_1\n      2016-06-16\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      4\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.153789\n      2.233399\n      2.424801\n    \n  \n\n\n\n\n\nY_test_df\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      575306\n      FOODS_1_035_CA_1\n      2016-06-13\n      0.0\n    \n    \n      575307\n      FOODS_1_035_CA_1\n      2016-06-14\n      1.0\n    \n    \n      575308\n      FOODS_1_035_CA_1\n      2016-06-15\n      0.0\n    \n    \n      575309\n      FOODS_1_035_CA_1\n      2016-06-16\n      4.0\n    \n    \n      575310\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      46805207\n      HOUSEHOLD_2_460_CA_1\n      2016-06-15\n      0.0\n    \n    \n      46805208\n      HOUSEHOLD_2_460_CA_1\n      2016-06-16\n      0.0\n    \n    \n      46805209\n      HOUSEHOLD_2_460_CA_1\n      2016-06-17\n      1.0\n    \n    \n      46805210\n      HOUSEHOLD_2_460_CA_1\n      2016-06-18\n      0.0\n    \n    \n      46805211\n      HOUSEHOLD_2_460_CA_1\n      2016-06-19\n      0.0\n    \n  \n\n700 rows √ó 3 columns"
  },
  {
    "objectID": "examples/IntermittentData.html#model-evaluation",
    "href": "examples/IntermittentData.html#model-evaluation",
    "title": "Forecasting Intermitent Time Series",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nQuantitative evaluation\nFor the evaluation we use the Mean Absolute Error\n\\[ \\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{N*H} \\sum_{i,\\tau} |y_{i,\\tau}-\\hat{y}_{i,\\tau}| \\]\n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\ny_true = Y_test_df['y'].values\nadida_preds = Y_hat_df['ADIDA'].values\nimapa_preds = Y_hat_df['IMAPA'].values\ntsb_preds = Y_hat_df['TSB'].values\n\nprint('ADIDA MAE: \\t %0.3f' % mae(adida_preds, y_true))\nprint('iMAPA MAE: \\t %0.3f' % mae(imapa_preds, y_true))\nprint('TSB   MAE: \\t %0.3f' % mae(tsb_preds, y_true))\n\nADIDA MAE:   1.172\niMAPA MAE:   1.173\nTSB   MAE:   1.180\n\n\n\n\nPlotting predictions\n\nY_hat_df\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      ADIDA\n      IMAPA\n      TSB\n    \n  \n  \n    \n      0\n      FOODS_1_035_CA_1\n      2016-06-13\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      1\n      FOODS_1_035_CA_1\n      2016-06-14\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      2\n      FOODS_1_035_CA_1\n      2016-06-15\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      3\n      FOODS_1_035_CA_1\n      2016-06-16\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      4\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      695\n      HOUSEHOLD_2_460_CA_1\n      2016-06-15\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      696\n      HOUSEHOLD_2_460_CA_1\n      2016-06-16\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      697\n      HOUSEHOLD_2_460_CA_1\n      2016-06-17\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      698\n      HOUSEHOLD_2_460_CA_1\n      2016-06-18\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      699\n      HOUSEHOLD_2_460_CA_1\n      2016-06-19\n      0.075524\n      0.059694\n      0.083630\n    \n  \n\n700 rows √ó 5 columns\n\n\n\n\nY_test_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n\nplot_grid(Y_train_df.groupby('unique_id').tail(3*horizon), \n          plot_titles=plot_titles, \n          model_cols=['ADIDA', 'IMAPA', 'TSB'], \n          df_test=Y_test_df)"
  },
  {
    "objectID": "examples/AutoArima_vs_Prophet.html#motivation",
    "href": "examples/AutoArima_vs_Prophet.html#motivation",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Motivation",
    "text": "Motivation\nThe AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the python implementation (pmdarima) is so slow that prevent data scientist practioners from quickly iterating and deploying AutoARIMA in production for a large number of time series. In this notebook we present Nixtla‚Äôs AutoARIMA based on the R implementation (developed by Rob Hyndman) and optimized using numba."
  },
  {
    "objectID": "examples/AutoArima_vs_Prophet.html#example",
    "href": "examples/AutoArima_vs_Prophet.html#example",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Example",
    "text": "Example\n\nLibraries\n\n!pip install statsforecast prophet statsmodels sklearn matplotlib pmdarima\n\n\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\nfrom multiprocessing import cpu_count, Pool # for prophet\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima as auto_arima_p\nfrom prophet import Prophet\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, _TS\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.model_selection import ParameterGrid\n\nImporting plotly failed. Interactive plots will not work.\n\n\n\nUseful functions\nThe plot_grid function defined below will be useful to plot different time series, and different models‚Äô forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for model in df_test.drop(['unique_id', 'ds'], axis=1).columns:\n                if all(np.isnan(test_uid[model])):\n                    continue\n                axes[idx, idy].plot(test_uid['ds'], test_uid[model], label=model)\n\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\ndef plot_autocorrelation_grid(df_train):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n\n    unique_ids = random.sample(list(unique_ids), k=8)\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        plot_acf(train_uid['y'].values, ax=axes[idx, idy], \n                 title=f'ACF M4 Hourly {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Autocorrelation')\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\n\n\nData\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\ntest = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 16\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nplot_grid(train, test)\n\n\n\n\nWould an autorregresive model be the right choice for our data? There is no doubt that we observe seasonal periods. The autocorrelation function (acf) can help us to answer the question. Intuitively, we have to observe a decreasing correlation to opt for an AR model.\n\nplot_autocorrelation_grid(train)\n\n\n\n\nThus, we observe a high autocorrelation for previous lags and also for the seasonal lags. Therefore, we will let auto_arima to handle our data.\n\n\nTraining and forecasting\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\n?AutoARIMA\n\n\nInit signature:\nAutoARIMA(\n    d: Optional[int] = None,\n    D: Optional[int] = None,\n    max_p: int = 5,\n    max_q: int = 5,\n    max_P: int = 2,\n    max_Q: int = 2,\n    max_order: int = 5,\n    max_d: int = 2,\n    max_D: int = 1,\n    start_p: int = 2,\n    start_q: int = 2,\n    start_P: int = 1,\n    start_Q: int = 1,\n    stationary: bool = False,\n    seasonal: bool = True,\n    ic: str = 'aicc',\n    stepwise: bool = True,\n    nmodels: int = 94,\n    trace: bool = False,\n    approximation: Optional[bool] = False,\n    method: Optional[str] = None,\n    truncate: Optional[bool] = None,\n    test: str = 'kpss',\n    test_kwargs: Optional[str] = None,\n    seasonal_test: str = 'seas',\n    seasonal_test_kwargs: Optional[Dict] = None,\n    allowdrift: bool = False,\n    allowmean: bool = False,\n    blambda: Optional[float] = None,\n    biasadj: bool = False,\n    parallel: bool = False,\n    num_cores: int = 2,\n    season_length: int = 1,\n)\nDocstring:      <no docstring>\nFile:           ~/fede/statsforecast/statsforecast/models.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we see, we can pass season_length to AutoARIMA, so the definition of our models would be,\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla = end - init\ntime_nixtla\n\n20.36360502243042\n\n\n\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      616.084167\n    \n    \n      H1\n      702\n      544.432129\n    \n    \n      H1\n      703\n      510.414490\n    \n    \n      H1\n      704\n      481.046539\n    \n    \n      H1\n      705\n      460.893066\n    \n  \n\n\n\n\n\nforecasts = forecasts.reset_index()\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)"
  },
  {
    "objectID": "examples/AutoArima_vs_Prophet.html#alternatives",
    "href": "examples/AutoArima_vs_Prophet.html#alternatives",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Alternatives",
    "text": "Alternatives\n\npmdarima\nYou can use the StatsForecast class to parallelize your own models. In this section we will use it to run the auto_arima model from pmdarima.\n\nclass PMDAutoARIMA(_TS):\n    \n    def __init__(self, season_length: int):\n        self.season_length = season_length\n        \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        mod = auto_arima_p(\n            y, m=self.season_length,\n            with_intercept=False #ensure comparability with Nixtla's implementation\n        ) \n        return {'mean': mod.predict(h)}\n    \n    def __repr__(self):\n        return 'pmdarima'\n\n\nn_series_pmdarima = 2\n\n\nfcst = StatsForecast(\n    df = train.query('unique_id in [\"H1\", \"H10\"]'), \n    models=[PMDAutoARIMA(season_length=24)],\n    freq='H',\n    n_jobs=-1\n)\n\n\ninit = time.time()\nforecast_pmdarima = fcst.forecast(48)\nend = time.time()\n\ntime_pmdarima = end - init\ntime_pmdarima\n\n349.93623208999634\n\n\n\nforecast_pmdarima.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      pmdarima\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      627.479370\n    \n    \n      H1\n      702\n      570.364380\n    \n    \n      H1\n      703\n      541.831482\n    \n    \n      H1\n      704\n      516.475647\n    \n    \n      H1\n      705\n      503.044586\n    \n  \n\n\n\n\n\nforecast_pmdarima = forecast_pmdarima.reset_index()\n\n\ntest = test.merge(forecast_pmdarima, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test, plot_random=False)\n\n\n\n\n\n\nProphet\nProphet is designed to receive a pandas dataframe, so we cannot use StatForecast. Therefore, we need to parallize from scratch.\n\nparams_grid = {'seasonality_mode': ['multiplicative','additive'],\n               'growth': ['linear', 'flat'], \n               'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5], \n               'n_changepoints': [5, 10, 15, 20]} \ngrid = ParameterGrid(params_grid)\n\n\ndef fit_and_predict(index, ts):\n    df = ts.drop(columns='unique_id', axis=1)\n    max_ds = df['ds'].max()\n    df['ds'] = pd.date_range(start='1970-01-01', periods=df.shape[0], freq='H')\n    df_val = df.tail(48) \n    df_train = df.drop(df_val.index) \n    y_val = df_val['y'].values\n    \n    if len(df_train) >= 48:\n        val_results = {'losses': [], 'params': []}\n\n        for params in grid:\n            model = Prophet(seasonality_mode=params['seasonality_mode'],\n                            growth=params['growth'],\n                            weekly_seasonality=True,\n                            daily_seasonality=True,\n                            yearly_seasonality=True,\n                            n_changepoints=params['n_changepoints'],\n                            changepoint_prior_scale=params['changepoint_prior_scale'])\n            model = model.fit(df_train)\n            \n            forecast = model.make_future_dataframe(periods=48, \n                                                   include_history=False, \n                                                   freq='H')\n            forecast = model.predict(forecast)\n            forecast['unique_id'] = index\n            forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n            \n            loss = np.mean(abs(y_val - forecast['yhat'].values))\n            \n            val_results['losses'].append(loss)\n            val_results['params'].append(params)\n\n        idx_params = np.argmin(val_results['losses']) \n        params = val_results['params'][idx_params]\n    else:\n        params = {'seasonality_mode': 'multiplicative',\n                  'growth': 'flat',\n                  'n_changepoints': 150,\n                  'changepoint_prior_scale': 0.5}\n    model = Prophet(seasonality_mode=params['seasonality_mode'],\n                    growth=params['growth'],\n                    weekly_seasonality=True,\n                    daily_seasonality=True,\n                    yearly_seasonality=True,\n                    n_changepoints=params['n_changepoints'],\n                    changepoint_prior_scale=params['changepoint_prior_scale'])\n    model = model.fit(df)\n    \n    forecast = model.make_future_dataframe(periods=48, \n                                           include_history=False, \n                                           freq='H')\n    forecast = model.predict(forecast)\n    forecast.insert(0, 'unique_id', index)\n    forecast['ds'] = np.arange(max_ds + 1, max_ds + 48 + 1)\n    forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n    \n    return forecast\n\n\nlogging.getLogger('prophet').setLevel(logging.WARNING)\n\n\nclass suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n\n\ninit = time.time()\nwith suppress_stdout_stderr():\n    with Pool(cpu_count()) as pool:\n        forecast_prophet = pool.starmap(fit_and_predict, train.groupby('unique_id'))\nend = time.time()\nforecast_prophet = pd.concat(forecast_prophet).rename(columns={'yhat': 'prophet'})\ntime_prophet = end - init\ntime_prophet\n\n2022-08-19 23:07:24 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:25 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:41 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:42 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:08:00 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n\n\n120.9244737625122\n\n\n\nforecast_prophet\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      prophet\n    \n  \n  \n    \n      0\n      H1\n      701\n      631.867439\n    \n    \n      1\n      H1\n      702\n      561.001661\n    \n    \n      2\n      H1\n      703\n      499.299334\n    \n    \n      3\n      H1\n      704\n      456.132082\n    \n    \n      4\n      H1\n      705\n      431.884528\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      H112\n      744\n      5634.503804\n    \n    \n      44\n      H112\n      745\n      5622.643542\n    \n    \n      45\n      H112\n      746\n      5546.302705\n    \n    \n      46\n      H112\n      747\n      5457.777165\n    \n    \n      47\n      H112\n      748\n      5373.944098\n    \n  \n\n768 rows √ó 3 columns\n\n\n\n\ntest = test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)\n\n\n\n\n\n\nEvaluation\n\n\nTime\nSince AutoARIMA works with numba is useful to calculate the time for just one time series.\n\nfcst = StatsForecast(df=train.query('unique_id == \"H1\"'), \n                     models=models, freq='H', \n                     n_jobs=1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla_1 = end - init\ntime_nixtla_1\n\n11.437001705169678\n\n\n\ntimes = pd.DataFrame({'n_series': np.arange(1, 414 + 1)})\ntimes['pmdarima'] = time_pmdarima * times['n_series'] / n_series_pmdarima\ntimes['prophet'] = time_prophet * times['n_series'] / n_series\ntimes['AutoARIMA_nixtla'] = time_nixtla_1 + times['n_series'] * (time_nixtla - time_nixtla_1) / n_series\ntimes = times.set_index('n_series')\n\n\ntimes.tail(5)\n\n\n\n\n\n  \n    \n      \n      pmdarima\n      prophet\n      AutoARIMA_nixtla\n    \n    \n      n_series\n      \n      \n      \n    \n  \n  \n    \n      410\n      71736.927578\n      3098.689640\n      240.181212\n    \n    \n      411\n      71911.895694\n      3106.247420\n      240.739124\n    \n    \n      412\n      72086.863811\n      3113.805199\n      241.297037\n    \n    \n      413\n      72261.831927\n      3121.362979\n      241.854950\n    \n    \n      414\n      72436.800043\n      3128.920759\n      242.412863\n    \n  \n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (24, 7))\n(times/3600).plot(ax=axes[0], linewidth=4)\nnp.log10(times).plot(ax=axes[1], linewidth=4)\naxes[0].set_title('Time across models [Hours]', fontsize=22)\naxes[1].set_title('Time across models [Log10 Scale]', fontsize=22)\naxes[0].set_ylabel('Time [Hours]', fontsize=20)\naxes[1].set_ylabel('Time Seconds [Log10 Scale]', fontsize=20)\nfig.suptitle('Time comparison using M4-Hourly data', fontsize=27)\nfor ax in axes:\n    ax.set_xlabel('Number of Time Series [N]', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid()\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(20)\n\n\n\n\n\nfig.savefig('computational-efficiency.png', dpi=300)\n\n\n\nPerformance\n\npmdarima (only two time series)\n\nname_models = test.drop(['unique_id', 'ds', 'y_test'], 1).columns.tolist()\n\n\ntest_pmdarima = test.query('unique_id in [\"H1\", \"H10\"]')\neval_pmdarima = []\nfor model in name_models:\n    mae = np.mean(abs(test_pmdarima[model] - test_pmdarima['y_test']))\n    eval_pmdarima.append({'model': model, 'mae': mae})\npd.DataFrame(eval_pmdarima).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      20.289669\n    \n    \n      1\n      pmdarima\n      26.461525\n    \n    \n      2\n      prophet\n      43.155861\n    \n  \n\n\n\n\n\n\nProphet\n\neval_prophet = []\nfor model in name_models:\n    if 'pmdarima' in model:\n        continue\n    mae = np.mean(abs(test[model] - test['y_test']))\n    eval_prophet.append({'model': model, 'mae': mae})\npd.DataFrame(eval_prophet).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      680.202970\n    \n    \n      1\n      prophet\n      1066.049049\n    \n  \n\n\n\n\nFor a complete comparison check the complete experiment."
  },
  {
    "objectID": "examples/CrossValidation.html",
    "href": "examples/CrossValidation.html",
    "title": "Cross-validation for Time Series",
    "section": "",
    "text": "Cross-Validation is a widely used technique in data science and machine learning. Most common cross-validation methods require shuffling of the data, and this is why those methods are not applicable to time-series data. The nature of time-series data requires a distinct approach to cross-validation.\nIn this notebook we will give intuition on how time-series cross-validation works. We will be using an example dataset and all the code required to implement the time-series cross-validation with StatsForecast."
  },
  {
    "objectID": "examples/CrossValidation.html#installing-statsforecast-library",
    "href": "examples/CrossValidation.html#installing-statsforecast-library",
    "title": "Cross-validation for Time Series",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install -U numba\n!pip install -U statsmodels\n!pip install statsforecast\n!pip install neuralforecast\n\n\nimport random\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom neuralforecast.losses.numpy import mqloss\nfrom sklearn import metrics\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\n\n\nAuxiliar plot functions\n\n# Auxiliar plot functions\ndef plot_cv_indices(cv_indices, total_obs, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    colors = {0:'tab:blue', 1:'tab:orange', 2:'tab:gray'}\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv_indices):\n        # Fill in indices with the training/test groups\n        indices = np.array([2] * total_obs)\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=pd.Series(indices).map(colors),\n            marker=\"_\",\n            lw=lw,\n            # cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Formatting\n    yticklabels = list(range(n_splits))\n    ax.set(\n        yticks=np.arange(n_splits) + 0.5,\n        yticklabels=yticklabels,\n        xlabel=\"Index\",\n        ylabel=\"CV iteration\",\n        ylim=[n_splits, -0.2],\n        xlim=[0, total_obs],\n    )\n    ax.set_title(\"Cross-validation splits\", fontsize=15)\n    return ax\n\ndef plot_grid(df_train, plot_titles, df_test=None, plot_random=True):\n    \"\"\"Plots multiple time series.\"\"\"\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train', c='black')\n        axes[idx, idy].xaxis.set_tick_params(rotation=45)\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y'], c='black', label='True')\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y_5'], c='blue', alpha=0.3)\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y_50'], c='blue', label='p50')\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y_95'], c='blue', alpha=0.3)\n            axes[idx, idy].fill_between(x=test_uid['ds'],\n                                        y1=test_uid['y_5'],\n                                        y2=test_uid['y_95'],\n                                        alpha=0.2, label='p5-p95')\n        axes[idx, idy].set_title(f'State: {plot_titles[uid]}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.7)\n    plt.show()"
  },
  {
    "objectID": "examples/CrossValidation.html#time-series-cross-validation",
    "href": "examples/CrossValidation.html#time-series-cross-validation",
    "title": "Cross-validation for Time Series",
    "section": "Time Series Cross-Validation",
    "text": "Time Series Cross-Validation\nIn this procedure there is a series of test sets. The corresponding training sets consist only of observations that ocurred prior to the observations from the test set. Thus, no future observations can be used in constructing the forecast. The following diagram illustrates the series of training and test sets, where the blue observations form the training sets, and the orange observations form the test sets. The forecast accuracy is computed by averaging over the test sets.\nAdapted from https://robjhyndman.com/hyndsight/tscv/\n\ntotal_obs = 100\nnfolds = 10\ncv_indices =[]\n\nfor i in range(1, nfolds+1)[::-1]:\n    cutoff = 100 - i*5\n    cv_indices.append((np.array(range(cutoff)), np.array(range(cutoff, cutoff+5))))\n\nfig, ax = plt.subplots()\nplot_cv_indices(cv_indices, total_obs, ax, nfolds)\n\n<AxesSubplot:title={'center':'Cross-validation splits'}, xlabel='Index', ylabel='CV iteration'>"
  },
  {
    "objectID": "examples/CrossValidation.html#loading-and-exploring-tourism-data",
    "href": "examples/CrossValidation.html#loading-and-exploring-tourism-data",
    "title": "Cross-validation for Time Series",
    "section": "Loading and Exploring Tourism Data",
    "text": "Loading and Exploring Tourism Data\nThe dataset we‚Äôll be using is a tourism dataset containing quarterly data on the number of trips performed during the time span. The trips are categorized by State, Region and Purpose. We‚Äôll be creating predictions at the State level.\n\ndata_url = \"https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv\"\ntourism_df = pd.read_csv(data_url, sep=\",\")\ntourism_df.head()\n\n\n\n\n\n  \n    \n      \n      Quarter\n      Region\n      State\n      Purpose\n      Trips\n    \n  \n  \n    \n      0\n      1998 Q1\n      Adelaide\n      South Australia\n      Business\n      135.077690\n    \n    \n      1\n      1998 Q2\n      Adelaide\n      South Australia\n      Business\n      109.987316\n    \n    \n      2\n      1998 Q3\n      Adelaide\n      South Australia\n      Business\n      166.034687\n    \n    \n      3\n      1998 Q4\n      Adelaide\n      South Australia\n      Business\n      127.160464\n    \n    \n      4\n      1999 Q1\n      Adelaide\n      South Australia\n      Business\n      137.448533\n    \n  \n\n\n\n\n\nPrepare dataset for StatsForecast modelling\n\n# Aggregate\ntarget_var = \"Trips\"\naggregation_vars = [\"Quarter\", \"State\"] \ntourism_df_agg = tourism_df[aggregation_vars + [target_var]].groupby(aggregation_vars, as_index=False).sum()\n\n# Create a dict maping string values in Quarter to datetime\nquarters = tourism_df[\"Quarter\"].sort_values().unique()\nds = pd.date_range(start='1998-01-01', periods=len(quarters), freq='Q')\nds_quarter = dict(zip(quarters, ds))\n\n# Prepare columns\ntourism_df_agg.rename(columns={target_var: 'y'}, inplace=True)\ntourism_df_agg[\"unique_id\"] = tourism_df_agg.groupby(\"State\").ngroup()\ntourism_df_agg[\"ds\"] = [ds_quarter[q] for q in tourism_df_agg[\"Quarter\"]]\n\nState_id = dict(zip(tourism_df_agg[\"unique_id\"].unique(), tourism_df_agg[\"State\"].unique()))\ntourism_df_agg.drop([\"Quarter\", \"State\"], axis=1, inplace=True)\n\n\nplot_grid(tourism_df_agg, plot_titles=State_id)"
  },
  {
    "objectID": "examples/CrossValidation.html#rolling-window-arima-predictions",
    "href": "examples/CrossValidation.html#rolling-window-arima-predictions",
    "title": "Cross-validation for Time Series",
    "section": "Rolling window ARIMA Predictions",
    "text": "Rolling window ARIMA Predictions\n\nPerform Cross-Validation\n\n# Modelling parameters\nseason_length = 4\nhorizon = 1\nfreq = \"Q\"\nn_windows_cv = 20\n\nCreate forecast object and perform cross validation.\n\nfcst = StatsForecast(\n    df=tourism_df_agg, \n    models=[AutoARIMA(season_length=season_length)], \n    freq=freq, \n    n_jobs=-1\n)\nforecasts_cv = fcst.cross_validation(h=horizon, n_windows=n_windows_cv, level=(90,)) \nforecasts_cv.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      2013-03-31\n      2012-12-31\n      524.553589\n      485.676483\n      382.970947\n      588.381958\n    \n    \n      0\n      2013-06-30\n      2013-03-31\n      475.532501\n      486.325409\n      384.189240\n      588.461609\n    \n    \n      0\n      2013-09-30\n      2013-06-30\n      506.512085\n      486.145721\n      384.876953\n      587.414490\n    \n    \n      0\n      2013-12-31\n      2013-09-30\n      529.584534\n      486.476532\n      385.993347\n      586.959717\n    \n    \n      0\n      2014-03-31\n      2013-12-31\n      540.607544\n      487.153168\n      387.096008\n      587.210327\n    \n  \n\n\n\n\nPlot time series CV.\n\ntime_cv_indices = [\n    (np.where(ds <= cutoff), np.where(ds == cutoff) + np.array(1)) \\\n    for cutoff in forecasts_cv[forecasts_cv.index == 0][\"cutoff\"]\n]\n\nfig, ax = plt.subplots()\nplot_cv_indices(time_cv_indices, tourism_df_agg[tourism_df_agg.unique_id == 0].shape[0], ax, n_windows_cv)\n\n<AxesSubplot:title={'center':'Cross-validation splits'}, xlabel='Index', ylabel='CV iteration'>\n\n\n\n\n\n\n\nQuantitative evaluation\nFor the evaluation we use the Mean Absolute Error:\n\\[ \\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{N*H} \\sum_{i,\\tau} |y_{i,\\tau}-\\hat{y}_{i,\\tau}| \\]\n\ndef compute_MAE(df, model_name):\n    return metrics.mean_absolute_error(df[\"y\"], df[model_name])\n\ncutoff_values = forecasts_cv[\"cutoff\"].unique()\ncv_MAE = []\nfor ct in cutoff_values:\n    cv_MAE.append(compute_MAE(forecasts_cv[forecasts_cv[\"cutoff\"] == ct], \"AutoARIMA\"))\nprint(f\"Average Mean Absolute Error across all Cross-Validation folds: {str(np.round(np.mean(cv_MAE), decimals=2))}\")\n\nAverage Mean Absolute Error across all Cross-Validation folds: 172.65\n\n\n\n\nPlotting predictions\n\nforecasts_cv.rename(\n    columns={\"AutoARIMA\": \"y_50\", \"AutoARIMA-lo-90\": \"y_5\", \"AutoARIMA-hi-90\": \"y_95\"}, \n    inplace=True\n)\nplot_grid(tourism_df_agg, plot_titles=State_id, df_test=forecasts_cv)"
  }
]